{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HNdarz1uIIZ"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/sherifmost/DeepLearning/blob/master/Labs/lab1/lab1_part1.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSA7aTOivRhA"
      },
      "source": [
        "**Parts of this lab are based on Kaggle kernels.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfATj8pltlxh"
      },
      "source": [
        "# Lab 1 - Part1: Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOy1mYdKw5TE"
      },
      "source": [
        "![Linear Regression](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/linear_regression.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsLKJ-4kwLYv"
      },
      "source": [
        "## 1.1.1 Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQxlb182u_gv"
      },
      "source": [
        "The problem we are trying to solve here is finding a new house which is suitable to our needs and the budget we assigned. The client who wants to buy the new house did her research and found some houses. She wrote the details of each house she visited including location, sale condition, sale type, house price, among others. She needs some help to know how much she is expected to pay to get a house that conforms with her specific needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUidwfyPvcfv"
      },
      "source": [
        "Your task is to build a linear regression model that helps her to predict the house price depending on the given attributes she collected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeXilc2Nvg-g"
      },
      "source": [
        "## 1.1.2 Problem Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsHAbLgL1MsY"
      },
      "source": [
        "Let's dive into the code, explain it and show you the parts you need to fill!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n5JS__dwfiH"
      },
      "source": [
        "### 1.1.2.1 Import Needed packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm9MPDVSsDpu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "from scipy.stats.stats import pearsonr\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EfciecZwm6L"
      },
      "source": [
        "### 1.1.2.2 Configure Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW1TyciwsN9_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PJSAiyx4HM"
      },
      "source": [
        "### 1.1.2.3 Work on the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcVsD6V1x-Ty"
      },
      "source": [
        "This dataset contains 80 features that demonstrate the state of the house and our target which is the house price.\n",
        "\n",
        "We begin by loading the train and test splits of the dataset using pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6zYNeXVvvRA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyl1lDq-yVFG"
      },
      "source": [
        "You can have a look at the train split of the dataset using the head command. I very much encourage you to have a deeper look on the dataset file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2JWObzgyY9t",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "05974589-ba5d-4de9-c7e0-42d0863663f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
              "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
              "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
              "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
              "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
              "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
              "\n",
              "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
              "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
              "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
              "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
              "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
              "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
              "\n",
              "  YrSold  SaleType  SaleCondition  SalePrice  \n",
              "0   2008        WD         Normal     208500  \n",
              "1   2007        WD         Normal     181500  \n",
              "2   2008        WD         Normal     223500  \n",
              "3   2006        WD        Abnorml     140000  \n",
              "4   2008        WD         Normal     250000  \n",
              "\n",
              "[5 rows x 81 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f85fd77-c7fb-4a15-b62f-066eb8edef1c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>...</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 81 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f85fd77-c7fb-4a15-b62f-066eb8edef1c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f85fd77-c7fb-4a15-b62f-066eb8edef1c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f85fd77-c7fb-4a15-b62f-066eb8edef1c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrXVoqAU0h1g"
      },
      "source": [
        "Data preprocessing:\n",
        "* First I'll transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal\n",
        "* Create Dummy variables for the categorical features\n",
        "* Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCCi1svksOj4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Concatenate all the data\n",
        "# We do this to be able to preprocess on the whole dataset\n",
        "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
        "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
        "\n",
        "# Log transform the target y in training data - by reference inside all\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
        "\n",
        "# Log transform skewed numeric features:\n",
        "\n",
        "# Get Numerical Fields\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index \n",
        "\n",
        "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewnessc\n",
        "skewed_feats = skewed_feats[skewed_feats > 0.75] # Get Skewed Columns\n",
        "skewed_feats = skewed_feats.index # Get Skewed Columns indices\n",
        "\n",
        "# Log scale skewed columns\n",
        "# Normalize the skewed distribution for better regression\n",
        "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
        "\n",
        "# Create Dummy variables for the categorical features \n",
        "all_data = pd.get_dummies(all_data) \n",
        "\n",
        "# Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "all_data = all_data.fillna(all_data.mean())\n",
        "\n",
        "# Split the data to training & testing\n",
        "X_train = all_data[:train.shape[0]]\n",
        "X_test = all_data[train.shape[0]:]\n",
        "y = train.SalePrice\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "# z = (x - u) / s\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "\n",
        "#split training data into training & validation, default splitting is 25% validation\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVa9K1Ga1vlo"
      },
      "source": [
        "### 1.1.2.4 Define your model here (TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IocosMrh2W3X"
      },
      "source": [
        "One important note you need to be aware of, linear regression is a neural network with only one perceptron (i.e. dense layer with one node) with a linear activation (i.e. no activation function). \n",
        "\n",
        "![One Perceptron Neural Network](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/perceptron.png)\n",
        "\n",
        "Use this note to define a **sequential model of one dense layer with one unit using Tensorflow.Keras**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh1NvkNXsVVG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([tf.keras.Input(shape=(X_tr.shape[1],)),\n",
        "  tf.keras.layers.Dense(1,\n",
        "  kernel_regularizer=tf.keras.regularizers.L1(0.1),\n",
        "  activity_regularizer=tf.keras.regularizers.L2(0.01))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSa178n-2BM-"
      },
      "source": [
        "### 1.1.2.5 Compile your model and print a summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg6Yy5oDsXyK",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4d2e2b-a085-4193-f6cb-430100ced385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 1)                 289       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 289\n",
            "Trainable params: 289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss = \"mean_squared_error\", optimizer = \"Adam\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FgD-Kqo3fwb"
      },
      "source": [
        "### 1.1.2.6 Train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA4pLcN73waK"
      },
      "source": [
        "Fit your model into the training data, use the validation data to be able to plot the loss decrement during the training. \n",
        "\n",
        "**Make sure to handle overfitting (check the [FAQ](https://github.com/sherifmost/DeepLearning/blob/master/Labs/lab1/FAQ.md))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAkW1KTm3TGU",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b00500-0b9f-41b7-f4c9-92af8b236fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "35/35 [==============================] - 1s 6ms/step - loss: 151.1022 - val_loss: 148.9691\n",
            "Epoch 2/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 149.0769 - val_loss: 149.5535\n",
            "Epoch 3/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 147.4292 - val_loss: 150.2297\n",
            "Epoch 4/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 145.8407 - val_loss: 150.8124\n",
            "Epoch 5/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 144.3638 - val_loss: 151.6880\n",
            "Epoch 6/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 143.0238 - val_loss: 152.4967\n",
            "Epoch 7/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 141.6973 - val_loss: 153.5933\n",
            "Epoch 8/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 140.2442 - val_loss: 154.6169\n",
            "Epoch 9/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 138.9765 - val_loss: 155.6785\n",
            "Epoch 10/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 137.6853 - val_loss: 156.8669\n",
            "Epoch 11/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 136.4351 - val_loss: 158.1815\n",
            "Epoch 12/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 135.2407 - val_loss: 159.4456\n",
            "Epoch 13/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 134.0644 - val_loss: 160.9508\n",
            "Epoch 14/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 132.9152 - val_loss: 162.4182\n",
            "Epoch 15/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 131.7975 - val_loss: 163.9585\n",
            "Epoch 16/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 130.6891 - val_loss: 165.6151\n",
            "Epoch 17/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 129.6102 - val_loss: 167.4280\n",
            "Epoch 18/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 128.5808 - val_loss: 169.1672\n",
            "Epoch 19/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 127.5110 - val_loss: 171.1216\n",
            "Epoch 20/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 126.4889 - val_loss: 173.0828\n",
            "Epoch 21/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 125.5448 - val_loss: 175.1220\n",
            "Epoch 22/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 124.4924 - val_loss: 177.1717\n",
            "Epoch 23/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 123.5266 - val_loss: 179.2345\n",
            "Epoch 24/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 122.5714 - val_loss: 181.5067\n",
            "Epoch 25/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 121.6182 - val_loss: 183.7981\n",
            "Epoch 26/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 120.6585 - val_loss: 186.1658\n",
            "Epoch 27/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 119.7311 - val_loss: 188.4491\n",
            "Epoch 28/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 118.7890 - val_loss: 190.9401\n",
            "Epoch 29/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 117.8850 - val_loss: 193.5768\n",
            "Epoch 30/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 116.9774 - val_loss: 196.3293\n",
            "Epoch 31/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 116.0799 - val_loss: 198.9470\n",
            "Epoch 32/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 115.2115 - val_loss: 201.6881\n",
            "Epoch 33/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 114.3472 - val_loss: 204.5567\n",
            "Epoch 34/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 113.4694 - val_loss: 207.4606\n",
            "Epoch 35/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 112.6070 - val_loss: 210.6104\n",
            "Epoch 36/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 111.7674 - val_loss: 213.5159\n",
            "Epoch 37/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 110.9379 - val_loss: 216.7040\n",
            "Epoch 38/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 110.1111 - val_loss: 219.9548\n",
            "Epoch 39/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 109.2649 - val_loss: 223.1399\n",
            "Epoch 40/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 108.4422 - val_loss: 226.5519\n",
            "Epoch 41/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 107.6422 - val_loss: 229.9356\n",
            "Epoch 42/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 106.8288 - val_loss: 233.3598\n",
            "Epoch 43/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 106.0514 - val_loss: 236.8444\n",
            "Epoch 44/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 105.2725 - val_loss: 240.4242\n",
            "Epoch 45/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 104.4304 - val_loss: 244.1860\n",
            "Epoch 46/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 103.6564 - val_loss: 247.9310\n",
            "Epoch 47/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 102.8868 - val_loss: 251.7426\n",
            "Epoch 48/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 102.1185 - val_loss: 255.7679\n",
            "Epoch 49/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 101.4302 - val_loss: 259.5881\n",
            "Epoch 50/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 100.6145 - val_loss: 263.6590\n",
            "Epoch 51/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 99.8786 - val_loss: 267.5339\n",
            "Epoch 52/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 99.1240 - val_loss: 271.7020\n",
            "Epoch 53/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 98.4072 - val_loss: 275.7143\n",
            "Epoch 54/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 97.6253 - val_loss: 280.0089\n",
            "Epoch 55/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 96.9207 - val_loss: 284.2799\n",
            "Epoch 56/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 96.1993 - val_loss: 288.7420\n",
            "Epoch 57/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 95.4648 - val_loss: 293.2064\n",
            "Epoch 58/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 94.8221 - val_loss: 297.8178\n",
            "Epoch 59/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 94.0471 - val_loss: 302.3322\n",
            "Epoch 60/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 93.3749 - val_loss: 306.9000\n",
            "Epoch 61/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 92.6605 - val_loss: 311.8138\n",
            "Epoch 62/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 91.9693 - val_loss: 316.2833\n",
            "Epoch 63/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 91.2559 - val_loss: 321.1312\n",
            "Epoch 64/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 90.5554 - val_loss: 325.8430\n",
            "Epoch 65/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 89.9077 - val_loss: 330.9120\n",
            "Epoch 66/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 89.2378 - val_loss: 335.8702\n",
            "Epoch 67/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 88.5748 - val_loss: 340.9128\n",
            "Epoch 68/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 87.8791 - val_loss: 345.9248\n",
            "Epoch 69/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 87.2151 - val_loss: 351.0777\n",
            "Epoch 70/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 86.5870 - val_loss: 356.2754\n",
            "Epoch 71/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 85.8869 - val_loss: 361.5115\n",
            "Epoch 72/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 85.2560 - val_loss: 366.4317\n",
            "Epoch 73/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 84.6334 - val_loss: 371.9195\n",
            "Epoch 74/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 83.9688 - val_loss: 377.3250\n",
            "Epoch 75/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 83.3311 - val_loss: 383.1059\n",
            "Epoch 76/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 82.6898 - val_loss: 388.1698\n",
            "Epoch 77/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 82.0931 - val_loss: 393.8389\n",
            "Epoch 78/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 81.4426 - val_loss: 399.3478\n",
            "Epoch 79/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 80.8660 - val_loss: 405.1322\n",
            "Epoch 80/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 80.2394 - val_loss: 410.9789\n",
            "Epoch 81/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 79.5893 - val_loss: 416.6171\n",
            "Epoch 82/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 78.9786 - val_loss: 422.5509\n",
            "Epoch 83/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 78.3866 - val_loss: 428.0247\n",
            "Epoch 84/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 77.7875 - val_loss: 434.1964\n",
            "Epoch 85/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 77.2055 - val_loss: 440.0154\n",
            "Epoch 86/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 76.5900 - val_loss: 445.9393\n",
            "Epoch 87/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 75.9959 - val_loss: 451.8253\n",
            "Epoch 88/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 75.4121 - val_loss: 458.2780\n",
            "Epoch 89/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 74.8013 - val_loss: 463.9576\n",
            "Epoch 90/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 74.2492 - val_loss: 470.2247\n",
            "Epoch 91/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 73.6664 - val_loss: 476.4183\n",
            "Epoch 92/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 73.1120 - val_loss: 482.6875\n",
            "Epoch 93/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 72.5033 - val_loss: 489.1629\n",
            "Epoch 94/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 71.9678 - val_loss: 495.6831\n",
            "Epoch 95/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 71.4126 - val_loss: 501.8898\n",
            "Epoch 96/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 70.8280 - val_loss: 508.0785\n",
            "Epoch 97/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 70.2923 - val_loss: 514.9244\n",
            "Epoch 98/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 69.7230 - val_loss: 521.5134\n",
            "Epoch 99/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 69.1968 - val_loss: 527.9263\n",
            "Epoch 100/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 68.6198 - val_loss: 534.6326\n",
            "Epoch 101/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 68.1080 - val_loss: 541.5515\n",
            "Epoch 102/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 67.5815 - val_loss: 547.7119\n",
            "Epoch 103/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 67.0336 - val_loss: 554.6757\n",
            "Epoch 104/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 66.4760 - val_loss: 561.5832\n",
            "Epoch 105/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 65.9604 - val_loss: 567.6226\n",
            "Epoch 106/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 65.4507 - val_loss: 574.4093\n",
            "Epoch 107/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 64.9126 - val_loss: 581.5864\n",
            "Epoch 108/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 64.3982 - val_loss: 588.6907\n",
            "Epoch 109/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 63.8904 - val_loss: 595.8605\n",
            "Epoch 110/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 63.3981 - val_loss: 602.3677\n",
            "Epoch 111/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 62.8902 - val_loss: 609.6556\n",
            "Epoch 112/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 62.3867 - val_loss: 616.7115\n",
            "Epoch 113/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 61.8853 - val_loss: 624.0454\n",
            "Epoch 114/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 61.3876 - val_loss: 630.9835\n",
            "Epoch 115/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 60.8759 - val_loss: 637.9753\n",
            "Epoch 116/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 60.3800 - val_loss: 645.0054\n",
            "Epoch 117/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.8876 - val_loss: 652.3643\n",
            "Epoch 118/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.4259 - val_loss: 659.6849\n",
            "Epoch 119/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 58.9389 - val_loss: 666.8281\n",
            "Epoch 120/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 58.4619 - val_loss: 674.4278\n",
            "Epoch 121/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 58.0162 - val_loss: 681.6612\n",
            "Epoch 122/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 57.5301 - val_loss: 689.1583\n",
            "Epoch 123/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 57.0710 - val_loss: 696.3727\n",
            "Epoch 124/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 56.6032 - val_loss: 703.6372\n",
            "Epoch 125/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 56.1405 - val_loss: 711.0250\n",
            "Epoch 126/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 55.6629 - val_loss: 718.4090\n",
            "Epoch 127/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 55.2137 - val_loss: 725.7082\n",
            "Epoch 128/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 54.8077 - val_loss: 733.1660\n",
            "Epoch 129/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 54.3296 - val_loss: 740.7734\n",
            "Epoch 130/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.8954 - val_loss: 748.5687\n",
            "Epoch 131/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.4409 - val_loss: 755.9388\n",
            "Epoch 132/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.0026 - val_loss: 763.2375\n",
            "Epoch 133/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 52.5678 - val_loss: 771.1903\n",
            "Epoch 134/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 52.1492 - val_loss: 778.7955\n",
            "Epoch 135/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 51.6922 - val_loss: 786.1420\n",
            "Epoch 136/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 51.2551 - val_loss: 793.9016\n",
            "Epoch 137/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 50.8286 - val_loss: 801.5707\n",
            "Epoch 138/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 50.4358 - val_loss: 808.9749\n",
            "Epoch 139/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 49.9959 - val_loss: 816.9003\n",
            "Epoch 140/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 49.5954 - val_loss: 824.7398\n",
            "Epoch 141/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 49.1825 - val_loss: 832.0414\n",
            "Epoch 142/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 48.7855 - val_loss: 839.8615\n",
            "Epoch 143/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 48.3586 - val_loss: 847.4777\n",
            "Epoch 144/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 47.9763 - val_loss: 854.9291\n",
            "Epoch 145/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 47.5484 - val_loss: 862.7678\n",
            "Epoch 146/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 47.1643 - val_loss: 870.7368\n",
            "Epoch 147/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.7804 - val_loss: 878.2772\n",
            "Epoch 148/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.3996 - val_loss: 885.8332\n",
            "Epoch 149/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.0297 - val_loss: 893.7795\n",
            "Epoch 150/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 45.6540 - val_loss: 901.3807\n",
            "Epoch 151/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 45.2508 - val_loss: 907.7534\n",
            "Epoch 152/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 44.8580 - val_loss: 915.6307\n",
            "Epoch 153/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 44.4826 - val_loss: 923.3854\n",
            "Epoch 154/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 44.1062 - val_loss: 931.0369\n",
            "Epoch 155/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 43.7286 - val_loss: 938.8613\n",
            "Epoch 156/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 43.3419 - val_loss: 946.4717\n",
            "Epoch 157/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.9968 - val_loss: 953.8688\n",
            "Epoch 158/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.6382 - val_loss: 961.4387\n",
            "Epoch 159/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.3083 - val_loss: 968.6271\n",
            "Epoch 160/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 41.9230 - val_loss: 976.5759\n",
            "Epoch 161/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 41.5713 - val_loss: 984.1513\n",
            "Epoch 162/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 41.2240 - val_loss: 991.5793\n",
            "Epoch 163/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.8697 - val_loss: 998.8931\n",
            "Epoch 164/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.5272 - val_loss: 1006.1464\n",
            "Epoch 165/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.1633 - val_loss: 1013.9799\n",
            "Epoch 166/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.8293 - val_loss: 1021.4487\n",
            "Epoch 167/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.5178 - val_loss: 1028.5541\n",
            "Epoch 168/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.1743 - val_loss: 1035.8080\n",
            "Epoch 169/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 38.8382 - val_loss: 1043.4735\n",
            "Epoch 170/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 38.5129 - val_loss: 1050.2443\n",
            "Epoch 171/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 38.1787 - val_loss: 1057.6707\n",
            "Epoch 172/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 37.8611 - val_loss: 1064.6135\n",
            "Epoch 173/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 37.5673 - val_loss: 1072.0439\n",
            "Epoch 174/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 37.2130 - val_loss: 1078.7461\n",
            "Epoch 175/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.8970 - val_loss: 1086.0295\n",
            "Epoch 176/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 36.5964 - val_loss: 1093.1034\n",
            "Epoch 177/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.2921 - val_loss: 1100.2593\n",
            "Epoch 178/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.9766 - val_loss: 1106.7957\n",
            "Epoch 179/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.6633 - val_loss: 1113.6536\n",
            "Epoch 180/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 35.3625 - val_loss: 1119.8850\n",
            "Epoch 181/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.0648 - val_loss: 1126.8878\n",
            "Epoch 182/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 34.7769 - val_loss: 1133.1057\n",
            "Epoch 183/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 34.4587 - val_loss: 1139.5563\n",
            "Epoch 184/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.1825 - val_loss: 1146.1823\n",
            "Epoch 185/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 33.8872 - val_loss: 1152.5942\n",
            "Epoch 186/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 33.6006 - val_loss: 1158.5789\n",
            "Epoch 187/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 33.3150 - val_loss: 1164.8204\n",
            "Epoch 188/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 33.0421 - val_loss: 1171.0197\n",
            "Epoch 189/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 32.7537 - val_loss: 1176.9932\n",
            "Epoch 190/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.4838 - val_loss: 1182.5902\n",
            "Epoch 191/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.2013 - val_loss: 1188.3208\n",
            "Epoch 192/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.9346 - val_loss: 1193.8625\n",
            "Epoch 193/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 31.6709 - val_loss: 1199.2560\n",
            "Epoch 194/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.4109 - val_loss: 1204.7456\n",
            "Epoch 195/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 31.1376 - val_loss: 1209.6993\n",
            "Epoch 196/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 30.8709 - val_loss: 1214.7662\n",
            "Epoch 197/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 30.6208 - val_loss: 1219.9164\n",
            "Epoch 198/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.3801 - val_loss: 1225.0273\n",
            "Epoch 199/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.1053 - val_loss: 1229.3000\n",
            "Epoch 200/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.8568 - val_loss: 1234.2618\n",
            "Epoch 201/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.5981 - val_loss: 1238.8120\n",
            "Epoch 202/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.3517 - val_loss: 1242.8699\n",
            "Epoch 203/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.1073 - val_loss: 1246.7804\n",
            "Epoch 204/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.8555 - val_loss: 1250.8652\n",
            "Epoch 205/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.6154 - val_loss: 1254.3075\n",
            "Epoch 206/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.3698 - val_loss: 1257.7467\n",
            "Epoch 207/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.1301 - val_loss: 1260.7933\n",
            "Epoch 208/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 27.8980 - val_loss: 1263.6859\n",
            "Epoch 209/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.6574 - val_loss: 1266.3203\n",
            "Epoch 210/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.4145 - val_loss: 1269.4189\n",
            "Epoch 211/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.1915 - val_loss: 1271.6139\n",
            "Epoch 212/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.9726 - val_loss: 1273.7258\n",
            "Epoch 213/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.7470 - val_loss: 1275.6882\n",
            "Epoch 214/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 26.5340 - val_loss: 1277.1022\n",
            "Epoch 215/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.3114 - val_loss: 1278.4164\n",
            "Epoch 216/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 26.0845 - val_loss: 1279.6898\n",
            "Epoch 217/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.8771 - val_loss: 1280.1472\n",
            "Epoch 218/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.6544 - val_loss: 1280.8092\n",
            "Epoch 219/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 25.4429 - val_loss: 1281.0417\n",
            "Epoch 220/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.2275 - val_loss: 1281.1257\n",
            "Epoch 221/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.0258 - val_loss: 1280.8347\n",
            "Epoch 222/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 24.8062 - val_loss: 1280.5819\n",
            "Epoch 223/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 24.5964 - val_loss: 1279.1005\n",
            "Epoch 224/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 24.3873 - val_loss: 1277.7378\n",
            "Epoch 225/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.1745 - val_loss: 1276.2349\n",
            "Epoch 226/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 23.9822 - val_loss: 1274.2424\n",
            "Epoch 227/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.7938 - val_loss: 1271.6053\n",
            "Epoch 228/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 23.5878 - val_loss: 1269.3442\n",
            "Epoch 229/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 23.3908 - val_loss: 1266.0217\n",
            "Epoch 230/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.1945 - val_loss: 1262.5898\n",
            "Epoch 231/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.0007 - val_loss: 1258.9594\n",
            "Epoch 232/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 22.8091 - val_loss: 1255.0775\n",
            "Epoch 233/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.6086 - val_loss: 1250.3571\n",
            "Epoch 234/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 22.4313 - val_loss: 1245.7345\n",
            "Epoch 235/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.2380 - val_loss: 1240.8728\n",
            "Epoch 236/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 22.0361 - val_loss: 1234.9550\n",
            "Epoch 237/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.8726 - val_loss: 1228.9764\n",
            "Epoch 238/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 21.6785 - val_loss: 1222.9539\n",
            "Epoch 239/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 21.4817 - val_loss: 1216.3083\n",
            "Epoch 240/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.3001 - val_loss: 1209.3668\n",
            "Epoch 241/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.1331 - val_loss: 1201.9517\n",
            "Epoch 242/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.9370 - val_loss: 1194.5985\n",
            "Epoch 243/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.7489 - val_loss: 1186.5985\n",
            "Epoch 244/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.5660 - val_loss: 1178.4792\n",
            "Epoch 245/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.3984 - val_loss: 1170.1404\n",
            "Epoch 246/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.2136 - val_loss: 1161.4916\n",
            "Epoch 247/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 20.0265 - val_loss: 1152.4532\n",
            "Epoch 248/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.8650 - val_loss: 1143.2249\n",
            "Epoch 249/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.6981 - val_loss: 1133.0386\n",
            "Epoch 250/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.5083 - val_loss: 1123.6593\n",
            "Epoch 251/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.3361 - val_loss: 1114.1927\n",
            "Epoch 252/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.1607 - val_loss: 1104.0770\n",
            "Epoch 253/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.9759 - val_loss: 1093.9655\n",
            "Epoch 254/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.7960 - val_loss: 1083.8972\n",
            "Epoch 255/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.6299 - val_loss: 1073.3718\n",
            "Epoch 256/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.4607 - val_loss: 1062.4600\n",
            "Epoch 257/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.2941 - val_loss: 1051.7100\n",
            "Epoch 258/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.1513 - val_loss: 1040.9010\n",
            "Epoch 259/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.9520 - val_loss: 1030.2511\n",
            "Epoch 260/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.8024 - val_loss: 1019.2859\n",
            "Epoch 261/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.6205 - val_loss: 1008.0857\n",
            "Epoch 262/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.4529 - val_loss: 996.8846\n",
            "Epoch 263/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.2866 - val_loss: 985.9294\n",
            "Epoch 264/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.1058 - val_loss: 974.5157\n",
            "Epoch 265/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.9423 - val_loss: 963.4783\n",
            "Epoch 266/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.7810 - val_loss: 952.2004\n",
            "Epoch 267/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.6245 - val_loss: 941.1539\n",
            "Epoch 268/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.4594 - val_loss: 930.1668\n",
            "Epoch 269/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.3029 - val_loss: 919.0321\n",
            "Epoch 270/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.1351 - val_loss: 907.6830\n",
            "Epoch 271/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 15.9773 - val_loss: 896.7438\n",
            "Epoch 272/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 15.8081 - val_loss: 885.6758\n",
            "Epoch 273/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.6467 - val_loss: 874.4124\n",
            "Epoch 274/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.4911 - val_loss: 863.2634\n",
            "Epoch 275/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.3351 - val_loss: 852.2337\n",
            "Epoch 276/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.1813 - val_loss: 841.1241\n",
            "Epoch 277/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.0235 - val_loss: 830.4158\n",
            "Epoch 278/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.8632 - val_loss: 819.4084\n",
            "Epoch 279/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.7112 - val_loss: 808.3405\n",
            "Epoch 280/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.5552 - val_loss: 797.5273\n",
            "Epoch 281/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.4105 - val_loss: 786.2955\n",
            "Epoch 282/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.2580 - val_loss: 775.6871\n",
            "Epoch 283/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.1063 - val_loss: 764.6384\n",
            "Epoch 284/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.9402 - val_loss: 754.2319\n",
            "Epoch 285/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.7961 - val_loss: 743.4390\n",
            "Epoch 286/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.6403 - val_loss: 732.6019\n",
            "Epoch 287/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.4897 - val_loss: 721.9355\n",
            "Epoch 288/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.3391 - val_loss: 711.2880\n",
            "Epoch 289/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.1894 - val_loss: 700.7106\n",
            "Epoch 290/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.0495 - val_loss: 690.0987\n",
            "Epoch 291/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9045 - val_loss: 679.4542\n",
            "Epoch 292/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.7536 - val_loss: 669.1661\n",
            "Epoch 293/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 12.6154 - val_loss: 658.5626\n",
            "Epoch 294/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.4612 - val_loss: 648.2584\n",
            "Epoch 295/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.3215 - val_loss: 637.7511\n",
            "Epoch 296/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 12.1778 - val_loss: 627.3979\n",
            "Epoch 297/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.0408 - val_loss: 617.1428\n",
            "Epoch 298/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.8900 - val_loss: 606.9645\n",
            "Epoch 299/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.7466 - val_loss: 596.6189\n",
            "Epoch 300/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.6023 - val_loss: 586.4082\n",
            "Epoch 301/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.4751 - val_loss: 576.4745\n",
            "Epoch 302/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.3237 - val_loss: 566.3912\n",
            "Epoch 303/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 11.1826 - val_loss: 556.2727\n",
            "Epoch 304/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 11.0433 - val_loss: 546.4362\n",
            "Epoch 305/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.9083 - val_loss: 536.4503\n",
            "Epoch 306/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.7731 - val_loss: 526.5046\n",
            "Epoch 307/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 10.6359 - val_loss: 516.6003\n",
            "Epoch 308/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.5026 - val_loss: 506.7849\n",
            "Epoch 309/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.3681 - val_loss: 497.1280\n",
            "Epoch 310/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.2300 - val_loss: 487.5187\n",
            "Epoch 311/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 10.1022 - val_loss: 477.7151\n",
            "Epoch 312/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.9690 - val_loss: 468.3056\n",
            "Epoch 313/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.8274 - val_loss: 458.7127\n",
            "Epoch 314/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.6924 - val_loss: 449.4054\n",
            "Epoch 315/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.5621 - val_loss: 439.9960\n",
            "Epoch 316/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 9.4394 - val_loss: 430.5696\n",
            "Epoch 317/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.3131 - val_loss: 421.6520\n",
            "Epoch 318/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.1717 - val_loss: 412.5035\n",
            "Epoch 319/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 9.0368 - val_loss: 403.4446\n",
            "Epoch 320/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.9188 - val_loss: 394.4070\n",
            "Epoch 321/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.7859 - val_loss: 385.7212\n",
            "Epoch 322/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.6575 - val_loss: 376.8414\n",
            "Epoch 323/500\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 8.5326 - val_loss: 368.1479\n",
            "Epoch 324/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.4053 - val_loss: 359.4269\n",
            "Epoch 325/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.2796 - val_loss: 350.9897\n",
            "Epoch 326/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.1572 - val_loss: 342.6235\n",
            "Epoch 327/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 8.0299 - val_loss: 334.4679\n",
            "Epoch 328/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.9102 - val_loss: 326.5772\n",
            "Epoch 329/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.7840 - val_loss: 318.4799\n",
            "Epoch 330/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.6598 - val_loss: 310.6238\n",
            "Epoch 331/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.5323 - val_loss: 302.9328\n",
            "Epoch 332/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.4153 - val_loss: 295.4739\n",
            "Epoch 333/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.2913 - val_loss: 288.0516\n",
            "Epoch 334/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.1681 - val_loss: 280.4923\n",
            "Epoch 335/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 7.0483 - val_loss: 273.2257\n",
            "Epoch 336/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.9284 - val_loss: 266.1545\n",
            "Epoch 337/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.8197 - val_loss: 259.1813\n",
            "Epoch 338/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.6924 - val_loss: 252.2722\n",
            "Epoch 339/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.5724 - val_loss: 245.6391\n",
            "Epoch 340/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.4581 - val_loss: 239.0413\n",
            "Epoch 341/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.3417 - val_loss: 232.5744\n",
            "Epoch 342/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.2225 - val_loss: 226.3456\n",
            "Epoch 343/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 6.1010 - val_loss: 220.1701\n",
            "Epoch 344/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.9932 - val_loss: 214.1257\n",
            "Epoch 345/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.8739 - val_loss: 208.3013\n",
            "Epoch 346/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.7579 - val_loss: 202.4066\n",
            "Epoch 347/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.6505 - val_loss: 196.8811\n",
            "Epoch 348/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.5366 - val_loss: 191.5113\n",
            "Epoch 349/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.4195 - val_loss: 186.2125\n",
            "Epoch 350/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 5.3095 - val_loss: 181.7961\n",
            "Epoch 351/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.2026 - val_loss: 177.7778\n",
            "Epoch 352/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.1048 - val_loss: 173.9636\n",
            "Epoch 353/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 5.0055 - val_loss: 170.2460\n",
            "Epoch 354/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.9087 - val_loss: 166.7135\n",
            "Epoch 355/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.8158 - val_loss: 163.1040\n",
            "Epoch 356/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 4.7288 - val_loss: 159.5984\n",
            "Epoch 357/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.6512 - val_loss: 156.3208\n",
            "Epoch 358/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.5577 - val_loss: 152.8807\n",
            "Epoch 359/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.4851 - val_loss: 149.6707\n",
            "Epoch 360/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.4107 - val_loss: 146.5535\n",
            "Epoch 361/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.3388 - val_loss: 143.4124\n",
            "Epoch 362/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.2742 - val_loss: 140.3363\n",
            "Epoch 363/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.2159 - val_loss: 137.2604\n",
            "Epoch 364/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.1577 - val_loss: 134.2209\n",
            "Epoch 365/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 4.0963 - val_loss: 131.2132\n",
            "Epoch 366/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 4.0412 - val_loss: 128.3035\n",
            "Epoch 367/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.9839 - val_loss: 125.3567\n",
            "Epoch 368/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.9287 - val_loss: 122.5199\n",
            "Epoch 369/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.8775 - val_loss: 119.6735\n",
            "Epoch 370/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.8212 - val_loss: 116.8423\n",
            "Epoch 371/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.7736 - val_loss: 114.0709\n",
            "Epoch 372/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.7192 - val_loss: 111.3966\n",
            "Epoch 373/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.6700 - val_loss: 108.7047\n",
            "Epoch 374/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.6221 - val_loss: 106.0049\n",
            "Epoch 375/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.5745 - val_loss: 103.4586\n",
            "Epoch 376/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.5263 - val_loss: 100.8676\n",
            "Epoch 377/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 3.4808 - val_loss: 98.3621\n",
            "Epoch 378/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.4354 - val_loss: 95.8403\n",
            "Epoch 379/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.3902 - val_loss: 93.4725\n",
            "Epoch 380/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.3482 - val_loss: 91.0665\n",
            "Epoch 381/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.3093 - val_loss: 88.6637\n",
            "Epoch 382/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 3.2639 - val_loss: 86.3447\n",
            "Epoch 383/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.2236 - val_loss: 84.0947\n",
            "Epoch 384/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.1819 - val_loss: 81.8606\n",
            "Epoch 385/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.1452 - val_loss: 79.6780\n",
            "Epoch 386/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.1054 - val_loss: 77.5410\n",
            "Epoch 387/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.0682 - val_loss: 75.4089\n",
            "Epoch 388/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 3.0322 - val_loss: 73.3209\n",
            "Epoch 389/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.9954 - val_loss: 71.3155\n",
            "Epoch 390/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.9597 - val_loss: 69.3193\n",
            "Epoch 391/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.9239 - val_loss: 67.3469\n",
            "Epoch 392/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.8916 - val_loss: 65.4552\n",
            "Epoch 393/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.8575 - val_loss: 63.5468\n",
            "Epoch 394/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.8266 - val_loss: 61.7374\n",
            "Epoch 395/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.7945 - val_loss: 59.9402\n",
            "Epoch 396/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.7621 - val_loss: 58.1684\n",
            "Epoch 397/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 2.7317 - val_loss: 56.4619\n",
            "Epoch 398/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.7013 - val_loss: 54.7332\n",
            "Epoch 399/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.6716 - val_loss: 53.0941\n",
            "Epoch 400/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.6432 - val_loss: 51.4767\n",
            "Epoch 401/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 2.6133 - val_loss: 49.8791\n",
            "Epoch 402/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.5871 - val_loss: 48.3437\n",
            "Epoch 403/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.5591 - val_loss: 46.8382\n",
            "Epoch 404/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.5315 - val_loss: 45.3592\n",
            "Epoch 405/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.5052 - val_loss: 43.9127\n",
            "Epoch 406/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.4799 - val_loss: 42.5127\n",
            "Epoch 407/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.4548 - val_loss: 41.1432\n",
            "Epoch 408/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.4295 - val_loss: 39.7958\n",
            "Epoch 409/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.4034 - val_loss: 38.4895\n",
            "Epoch 410/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.3814 - val_loss: 37.2334\n",
            "Epoch 411/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.3564 - val_loss: 35.9836\n",
            "Epoch 412/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.3327 - val_loss: 34.7808\n",
            "Epoch 413/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.3098 - val_loss: 33.6104\n",
            "Epoch 414/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.2870 - val_loss: 32.4670\n",
            "Epoch 415/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.2643 - val_loss: 31.3565\n",
            "Epoch 416/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 2.2424 - val_loss: 30.2772\n",
            "Epoch 417/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 2.2217 - val_loss: 29.2249\n",
            "Epoch 418/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 2.1995 - val_loss: 28.2253\n",
            "Epoch 419/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.1789 - val_loss: 27.2402\n",
            "Epoch 420/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.1585 - val_loss: 26.2733\n",
            "Epoch 421/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.1378 - val_loss: 25.3434\n",
            "Epoch 422/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.1194 - val_loss: 24.4561\n",
            "Epoch 423/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0990 - val_loss: 23.5873\n",
            "Epoch 424/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0788 - val_loss: 22.7526\n",
            "Epoch 425/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0595 - val_loss: 21.9571\n",
            "Epoch 426/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 2.0394 - val_loss: 21.1854\n",
            "Epoch 427/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0206 - val_loss: 20.4355\n",
            "Epoch 428/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 2.0032 - val_loss: 19.7011\n",
            "Epoch 429/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9921 - val_loss: 19.0147\n",
            "Epoch 430/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.9813 - val_loss: 18.3390\n",
            "Epoch 431/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9699 - val_loss: 17.6714\n",
            "Epoch 432/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9593 - val_loss: 17.0328\n",
            "Epoch 433/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9505 - val_loss: 16.3969\n",
            "Epoch 434/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.9390 - val_loss: 15.7678\n",
            "Epoch 435/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9293 - val_loss: 15.1689\n",
            "Epoch 436/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9198 - val_loss: 14.5757\n",
            "Epoch 437/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9116 - val_loss: 14.0019\n",
            "Epoch 438/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.9009 - val_loss: 13.4351\n",
            "Epoch 439/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.8921 - val_loss: 12.8925\n",
            "Epoch 440/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8831 - val_loss: 12.3537\n",
            "Epoch 441/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8735 - val_loss: 11.8409\n",
            "Epoch 442/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.8648 - val_loss: 11.3251\n",
            "Epoch 443/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8556 - val_loss: 10.8319\n",
            "Epoch 444/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.8470 - val_loss: 10.3614\n",
            "Epoch 445/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.8391 - val_loss: 9.8997\n",
            "Epoch 446/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.8302 - val_loss: 9.4443\n",
            "Epoch 447/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.8226 - val_loss: 9.0119\n",
            "Epoch 448/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8131 - val_loss: 8.5890\n",
            "Epoch 449/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.8059 - val_loss: 8.1863\n",
            "Epoch 450/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.7973 - val_loss: 7.7813\n",
            "Epoch 451/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7895 - val_loss: 7.3997\n",
            "Epoch 452/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7809 - val_loss: 7.0314\n",
            "Epoch 453/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7729 - val_loss: 6.6715\n",
            "Epoch 454/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7653 - val_loss: 6.3298\n",
            "Epoch 455/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7574 - val_loss: 5.9941\n",
            "Epoch 456/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.7491 - val_loss: 5.6751\n",
            "Epoch 457/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7419 - val_loss: 5.3717\n",
            "Epoch 458/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.7340 - val_loss: 5.0757\n",
            "Epoch 459/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7261 - val_loss: 4.7984\n",
            "Epoch 460/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7185 - val_loss: 4.5242\n",
            "Epoch 461/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7108 - val_loss: 4.2682\n",
            "Epoch 462/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.7032 - val_loss: 4.0246\n",
            "Epoch 463/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6954 - val_loss: 3.7874\n",
            "Epoch 464/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6874 - val_loss: 3.5714\n",
            "Epoch 465/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6807 - val_loss: 3.3614\n",
            "Epoch 466/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6726 - val_loss: 3.1613\n",
            "Epoch 467/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6660 - val_loss: 2.9770\n",
            "Epoch 468/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6581 - val_loss: 2.8046\n",
            "Epoch 469/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6520 - val_loss: 2.6390\n",
            "Epoch 470/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6431 - val_loss: 2.4867\n",
            "Epoch 471/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6357 - val_loss: 2.3488\n",
            "Epoch 472/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6290 - val_loss: 2.2185\n",
            "Epoch 473/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6204 - val_loss: 2.1036\n",
            "Epoch 474/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.6133 - val_loss: 1.9959\n",
            "Epoch 475/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.6063 - val_loss: 1.9022\n",
            "Epoch 476/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5989 - val_loss: 1.8183\n",
            "Epoch 477/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5911 - val_loss: 1.7442\n",
            "Epoch 478/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5837 - val_loss: 1.6826\n",
            "Epoch 479/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5767 - val_loss: 1.6327\n",
            "Epoch 480/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5695 - val_loss: 1.5928\n",
            "Epoch 481/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5621 - val_loss: 1.5651\n",
            "Epoch 482/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5548 - val_loss: 1.5473\n",
            "Epoch 483/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.5484 - val_loss: 1.5402\n",
            "Epoch 484/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5465 - val_loss: 1.5398\n",
            "Epoch 485/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5461 - val_loss: 1.5411\n",
            "Epoch 486/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5466 - val_loss: 1.5412\n",
            "Epoch 487/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5474 - val_loss: 1.5403\n",
            "Epoch 488/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5460 - val_loss: 1.5407\n",
            "Epoch 489/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5467 - val_loss: 1.5401\n",
            "Epoch 490/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5462 - val_loss: 1.5396\n",
            "Epoch 491/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.5469 - val_loss: 1.5398\n",
            "Epoch 492/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5469 - val_loss: 1.5393\n",
            "Epoch 493/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5466 - val_loss: 1.5403\n",
            "Epoch 494/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5462 - val_loss: 1.5389\n",
            "Epoch 495/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5461 - val_loss: 1.5410\n",
            "Epoch 496/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5464 - val_loss: 1.5400\n",
            "Epoch 497/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5472 - val_loss: 1.5402\n",
            "Epoch 498/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5480 - val_loss: 1.5404\n",
            "Epoch 499/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 1.5479 - val_loss: 1.5405\n",
            "Epoch 500/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 1.5460 - val_loss: 1.5419\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtdUbgUd4Ifw"
      },
      "source": [
        "And this is how you can predict an output for any number of inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaO-wCLl3Wd_",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290331a0-12f7-426e-df8e-c08db7c9c645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[32.46409 ]\n",
            " [31.38329 ]\n",
            " [32.13869 ]\n",
            " ...\n",
            " [32.163395]\n",
            " [30.543072]\n",
            " [32.9493  ]]\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6VhmPDN4Sva"
      },
      "source": [
        "### 1.1.2.7 Visualize Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bDCQffM4Zh2"
      },
      "source": [
        "It's time to see how your model's progress during the training, If all is good, you will find the validation loss decreasing without neither overfitting nor underfitting.\n",
        "\n",
        "Note that the loss here is used to indicate overfitting or underfitting, but loss shouldn't be regarded as an evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEZ7SdwI2e_D",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "4b7f03c3-65eb-4335-a6d7-f7ef9bc24961"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 395,
              "height": 261
            },
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Get training and test loss histories\n",
        "training_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.figure()\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, val_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gni9AWDueGU"
      },
      "source": [
        "## 1.1.3 Conclusion:\n",
        "\n",
        "That's it! Congratulations on training a linear regression model. \n",
        "\n",
        "Make sure you finish the second part of the assignment and deliver all the requirements for the submission.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}